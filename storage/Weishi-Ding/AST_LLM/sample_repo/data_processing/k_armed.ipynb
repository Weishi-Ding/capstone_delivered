{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k3hXhSYMoOcP"
      },
      "outputs": [],
      "source": [
        "#Author: Zichen Yang\n",
        "#Date: 09-17-2023\n",
        "#Assignment: RL HW01\n",
        "\n",
        "import numpy as np\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "std_dev   = 1\n",
        "rw_dev    = 0.01\n",
        "std_mean  = 0\n",
        "step_size = 0.1\n",
        "epsilon   = 0.1   ##prbability of exploration\n",
        "decay_rate = 0.00008\n",
        "num_runs = 2000\n",
        "num_steps = 10000"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fxinjos_oOcQ"
      },
      "source": [
        "# Environment Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t66Nm8IXoOcR"
      },
      "outputs": [],
      "source": [
        "class Environment:\n",
        "    def __init__(self, k = 10):\n",
        "        ###sample_average\n",
        "        self.num_arms = k\n",
        "        self.qs = np.zeros(k)          ##This is q*(a)\n",
        "\n",
        "    ##Reset function is used to create a new k-armed Testbed\n",
        "    def reset(self):\n",
        "        self.qs = self.qs = np.zeros(self.num_arms)           ##This is q*(a)\n",
        "\n",
        "\n",
        "    ##Call this function to take one action\n",
        "    def bandit(self, action):\n",
        "        reward = np.random.normal(self.qs[action], std_dev)\n",
        "        for i in range(self.num_arms):\n",
        "            self.qs[i] +=  np.random.normal(std_mean, rw_dev) ##update q*(a)\n",
        "        return reward\n",
        "\n",
        "    def get_optimal_actions(self):\n",
        "        max_val = max(self.qs)\n",
        "        indices = [i for i, x in enumerate(self.qs) if x == max_val]\n",
        "        return indices\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1gam6u7loOcS"
      },
      "source": [
        "# Agent Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z4V4gVe0oOcS"
      },
      "outputs": [],
      "source": [
        "class Agent:\n",
        "    def __init__(self, k = 10):\n",
        "        self.num_arms = k\n",
        "        self.Q_values = np.zeros(k)  ##This is Q(A)\n",
        "        self.action_count = np.zeros(k)  ##This is N(A)\n",
        "\n",
        "        ###Gradient-Bandit\n",
        "        self.performances = np.zeros(k)  ##This is performance\n",
        "        self.rewards = [0]           ##Initialize as [0] to avoid NaN\n",
        "\n",
        "\n",
        "    def reset(self):\n",
        "        self.Q_values = np.zeros(self.num_arms)  ##This is Q(A)\n",
        "        self.action_count = np.zeros(self.num_arms)  ##This is N(A)\n",
        "\n",
        "        ###Gradient-Bandit\n",
        "        self.performances = np.zeros(self.num_arms)  ##This is performance\n",
        "        self.rewards = [0]\n",
        "\n",
        "    def pick_action_greedy(self):\n",
        "        max_val = max(self.Q_values)\n",
        "        indices = [i for i, x in enumerate(self.Q_values) if x == max_val]\n",
        "        return random.choice(indices)\n",
        "\n",
        "    ##Call this function to pick one action using sample-average\n",
        "    def pick_action_eg(self, epsilon):\n",
        "        num = random.random()\n",
        "        if num <= epsilon:\n",
        "            action = np.random.randint(0, self.num_arms)\n",
        "            return action\n",
        "        else:\n",
        "            return self.pick_action_greedy()\n",
        "\n",
        "    def pick_action_ucb(self, c, time_step):\n",
        "        ucb_values = self.Q_values + c * np.sqrt(np.log(time_step + 1) / (self.action_count + 1e-5))\n",
        "        max_val = max(ucb_values)\n",
        "        indices = [i for i, x in enumerate(ucb_values) if x == max_val]\n",
        "        return random.choice(indices)\n",
        "\n",
        "    ##Call this function to pick one action using gradient-bandit\n",
        "    def pick_action_gb(self):\n",
        "        prob_a = np.exp(self.performances) / np.sum(np.exp(self.performances))\n",
        "        action = np.random.choice([x for x in range(self.num_arms)], p=prob_a)\n",
        "        return actionper\n",
        "\n",
        "     ##Call this function to update the true values and action counts using sample average method\n",
        "    def sample_average_update(self, action, reward):\n",
        "        self.action_count[action] += 1\n",
        "        self.Q_values[action] = self.Q_values[action] + (1 / self.action_count[action]) * (reward - self.Q_values[action])\n",
        "        self.rewards.append(reward)\n",
        "\n",
        "    def step_size_update(self, step_size, action, reward):\n",
        "        self.action_count[action] += 1\n",
        "        self.Q_values[action] = self.Q_values[action] + step_size * (reward - self.Q_values[action])\n",
        "        self.rewards.append(reward)\n",
        "\n",
        "    def increase_step_size_update(self, decay_rate, step_size, action, reward, time_step):\n",
        "        new_step_size = step_size / (1 - decay_rate * time_step)\n",
        "        self.step_size_update(new_step_size, action, reward)\n",
        "\n",
        "    ##Call this function to update the true values and action counts using gradient method\n",
        "    def gradient_update(self, action, reward):\n",
        "        prob_a = np.exp(self.performances) / np.sum(np.exp(self.performances))\n",
        "\n",
        "        for i in range(self.num_arms):\n",
        "            if action == i:\n",
        "                self.performances[i] += step_size * (reward - np.mean(self.rewards)) * (1 - prob_a[i])\n",
        "            else:\n",
        "                self.performances[i] -= step_size * (reward - np.mean(self.rewards)) * prob_a[i]\n",
        "        self.rewards.append(reward)\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yO9ym3TcoOcS"
      },
      "source": [
        "# Main Program"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k-wmtu-CoOcT"
      },
      "outputs": [],
      "source": [
        "#### sample-average method with greedy as the bench mark\n",
        "player = Agent()     ##Create an agent\n",
        "game = Environment() ##Create a game\n",
        "sum_ave_rewards = np.zeros(num_steps)\n",
        "sum_opt_per     = np.zeros(num_steps)\n",
        "\n",
        "for i in range(num_runs):\n",
        "    opt_list = []\n",
        "    for j in range(num_steps):\n",
        "        opt_actions = game.get_optimal_actions()\n",
        "        action = player.pick_action_greedy()\n",
        "        if action in opt_actions:\n",
        "            opt_list.append(1)\n",
        "        else:\n",
        "            opt_list.append(0)\n",
        "        reward = game.bandit(action)\n",
        "        player.sample_average_update(action, reward)\n",
        "\n",
        "    ave_rewards = np.cumsum(player.rewards[1:]) / np.arange(1, num_steps + 1)\n",
        "    opt_per = np.cumsum(opt_list) / np.arange(1, num_steps + 1) * 100\n",
        "\n",
        "    sum_ave_rewards += ave_rewards\n",
        "    sum_opt_per += opt_per\n",
        "\n",
        "    player.reset()\n",
        "    game.reset()\n",
        "\n",
        "overall_ave_rewards = sum_ave_rewards / num_runs\n",
        "overall_opt_per     = sum_opt_per / num_runs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cuhBy7aloOcT"
      },
      "outputs": [],
      "source": [
        "#### constant step-size method with UCB\n",
        "player2 = Agent()     ##Create an agent\n",
        "game2 = Environment() ##Create a game\n",
        "sum_ave_rewards2 = np.zeros(num_steps)\n",
        "sum_opt_per2     = np.zeros(num_steps)\n",
        "\n",
        "for i in range(num_runs):\n",
        "    opt_list = []\n",
        "    for j in range(num_steps):\n",
        "        opt_actions = game2.get_optimal_actions()\n",
        "        action = player2.pick_action_ucb(epsilon, j)\n",
        "        if action in opt_actions:\n",
        "            opt_list.append(1)\n",
        "        else:\n",
        "            opt_list.append(0)\n",
        "        reward = game2.bandit(action)\n",
        "        player2.step_size_update(step_size, action, reward)\n",
        "\n",
        "    ave_rewards = np.cumsum(player2.rewards[1:]) / np.arange(1, num_steps + 1)\n",
        "    opt_per = np.cumsum(opt_list) / np.arange(1, num_steps + 1) * 100\n",
        "\n",
        "    sum_ave_rewards2 += ave_rewards\n",
        "    sum_opt_per2 += opt_per\n",
        "\n",
        "    player2.reset()\n",
        "    game2.reset()\n",
        "\n",
        "overall_ave_rewards2 = sum_ave_rewards2 / num_runs\n",
        "overall_opt_per2     = sum_opt_per2 / num_runs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OkD3fcpBoOcT"
      },
      "outputs": [],
      "source": [
        "#### sample-average method with epsilon greedy\n",
        "player3 = Agent()     ##Create an agent\n",
        "game3 = Environment() ##Create a game\n",
        "sum_ave_rewards3 = np.zeros(num_steps)\n",
        "sum_opt_per3     = np.zeros(num_steps)\n",
        "\n",
        "for i in range(num_runs):\n",
        "    opt_list = []\n",
        "    for j in range(num_steps):\n",
        "        opt_actions = game3.get_optimal_actions()\n",
        "        action = player3.pick_action_eg(epsilon)\n",
        "        if action in opt_actions:\n",
        "            opt_list.append(1)\n",
        "        else:\n",
        "            opt_list.append(0)\n",
        "        reward = game3.bandit(action)\n",
        "        player3.sample_average_update(action, reward)\n",
        "\n",
        "    ave_rewards = np.cumsum(player3.rewards[1:]) / np.arange(1, num_steps + 1)\n",
        "    opt_per = np.cumsum(opt_list) / np.arange(1, num_steps + 1) * 100\n",
        "\n",
        "    sum_ave_rewards3 += ave_rewards\n",
        "    sum_opt_per3 += opt_per\n",
        "\n",
        "    player3.reset()\n",
        "    game3.reset()\n",
        "\n",
        "overall_ave_rewards3 = sum_ave_rewards3 / num_runs\n",
        "overall_opt_per3     = sum_opt_per3 / num_runs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ssB0wuxpoOcU"
      },
      "outputs": [],
      "source": [
        "##increasing step-size with UCB\n",
        "player4 = Agent()     ##Create an agent\n",
        "game4 = Environment() ##Create a game\n",
        "sum_ave_rewards4 = np.zeros(num_steps)\n",
        "sum_opt_per4     = np.zeros(num_steps)\n",
        "\n",
        "for i in range(num_runs):\n",
        "    opt_list = []\n",
        "    for j in range(num_steps):\n",
        "        opt_actions = game4.get_optimal_actions()\n",
        "        action = player4.pick_action_ucb(epsilon,j)\n",
        "        if action in opt_actions:\n",
        "            opt_list.append(1)\n",
        "        else:\n",
        "            opt_list.append(0)\n",
        "        reward = game4.bandit(action)\n",
        "        player4.increase_step_size_update(decay_rate, step_size, action, reward, j)\n",
        "\n",
        "    ave_rewards = np.cumsum(player4.rewards[1:]) / np.arange(1, num_steps + 1)\n",
        "    opt_per = np.cumsum(opt_list) / np.arange(1, num_steps + 1) * 100\n",
        "\n",
        "    sum_ave_rewards4 += ave_rewards\n",
        "    sum_opt_per4 += opt_per\n",
        "\n",
        "    player4.reset()\n",
        "    game4.reset()\n",
        "\n",
        "overall_ave_rewards4 = sum_ave_rewards4 / num_runs\n",
        "overall_opt_per4     = sum_opt_per4 / num_runs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q7frIX63oOcU"
      },
      "outputs": [],
      "source": [
        "##gradient bandit\n",
        "player5 = Agent()     ##Create an agent\n",
        "game5 = Environment() ##Create a game\n",
        "sum_ave_rewards5 = np.zeros(num_steps)\n",
        "sum_opt_per5     = np.zeros(num_steps)\n",
        "\n",
        "for i in range(num_runs):\n",
        "    opt_list = []\n",
        "    for j in range(num_steps):\n",
        "        opt_actions = game5.get_optimal_actions()\n",
        "        action = player5.pick_action_gb()\n",
        "        if action in opt_actions:\n",
        "            opt_list.append(1)\n",
        "        else:\n",
        "            opt_list.append(0)\n",
        "        reward = game5.bandit(action)\n",
        "        player5.gradient_update(action, reward)\n",
        "\n",
        "    ave_rewards = np.cumsum(player5.rewards[1:]) / np.arange(1, num_steps + 1)\n",
        "    opt_per = np.cumsum(opt_list) / np.arange(1, num_steps + 1) * 100\n",
        "\n",
        "    sum_ave_rewards5 += ave_rewards\n",
        "    sum_opt_per5 += opt_per\n",
        "\n",
        "    player5.reset()\n",
        "    game5.reset()\n",
        "\n",
        "overall_ave_rewards5 = sum_ave_rewards5 / num_runs\n",
        "overall_opt_per5     = sum_opt_per5 / num_runs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rGNGUbNUoOcU"
      },
      "source": [
        "# Graphs for the Main Experiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p5n2OZ6soOcU"
      },
      "outputs": [],
      "source": [
        "###Plot graphs\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(np.arange(num_steps), overall_ave_rewards, label = \"sample-average with ε-greedy(ε=0)\")\n",
        "plt.plot(np.arange(num_iterations), overall_ave_rewards2, label = \"constant step-size with UCB\")\n",
        "plt.plot(np.arange(num_iterations), overall_ave_rewards3, label = \"sample-average with ε-greedy(ε=0.1)\")\n",
        "plt.xticks([0, 2000, 4000, 6000, 8000, 10000], [\"0\", \"2000\", \"4000\", \"6000\", \"8000\", \"10000\"], fontsize = 12)\n",
        "plt.xlabel(\"Steps\", fontsize = 15)\n",
        "plt.ylabel(\"Average\\nreward\", fontsize = 15, rotation=\"horizontal\", labelpad=40)\n",
        "plt.title(\"Average Reward Over 2000 Runs\", fontsize = 20)\n",
        "plt.legend(fontsize = 16)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zGFqs87loOcV"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(np.arange(num_iterations), overall_opt_per, label = \"sample-average with ε-greedy(ε=0)\")\n",
        "plt.plot(np.arange(num_iterations), overall_opt_per2, label = \"constant step-size with UCB\")\n",
        "plt.plot(np.arange(num_iterations), overall_opt_per3, label = \"sample-average with ε-greedy(ε=0.1)\")\n",
        "plt.xticks([0, 2000, 4000, 6000, 8000, 10000], [\"0\", \"2000\", \"4000\", \"6000\", \"8000\", \"10000\"], fontsize = 12)\n",
        "plt.yticks([20, 40, 60, 80, 100], [\"20%\", \"40%\", \"60%\", \"80%\", \"100%\"], fontsize = 12)\n",
        "plt.xlabel(\"Steps\", fontsize = 15)\n",
        "plt.ylabel(\"%Optimal\\naction\", fontsize = 15, rotation=\"horizontal\", labelpad=40)\n",
        "plt.title(\"Average %Optimal_Action Over 2000 Runs\", fontsize = 20)\n",
        "plt.legend(fontsize = 16)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ITEigrtCoOcV"
      },
      "source": [
        "# Graphs for Additional Questions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A9FEQ7XuoOcV"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(np.arange(num_steps), overall_ave_rewards, label = \"sample-average with ε-greedy(ε=0)\")\n",
        "plt.plot(np.arange(num_iterations), overall_ave_rewards2, label = \"constant step-size with UCB\")\n",
        "plt.plot(np.arange(num_iterations), overall_ave_rewards3, label = \"sample-average with ε-greedy(ε=0.1)\")\n",
        "plt.plot(np.arange(num_iterations), overall_ave_rewards4, label = \"constant step-size with UCB\")\n",
        "plt.plot(np.arange(num_iterations), overall_ave_rewards5, label = \"Gradient-Bandit Algorithm\")\n",
        "plt.xticks([0, 2000, 4000, 6000, 8000, 10000], [\"0\", \"2000\", \"4000\", \"6000\", \"8000\", \"10000\"], fontsize = 12)\n",
        "plt.xlabel(\"Steps\", fontsize = 15)\n",
        "plt.ylabel(\"Average\\nreward\", fontsize = 15, rotation=\"horizontal\", labelpad=40)\n",
        "plt.title(\"Average Reward Over 2000 Runs\", fontsize = 20)\n",
        "plt.legend(fontsize = 16)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NNgOO8BIoOcW"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(np.arange(num_iterations), overall_opt_per, label = \"sample-average with ε-greedy(ε=0)\")\n",
        "plt.plot(np.arange(num_iterations), overall_opt_per2, label = \"constant step-size with UCB\")\n",
        "plt.plot(np.arange(num_iterations), overall_opt_per3, label = \"sample-average with ε-greedy(ε=0.1)\")\n",
        "plt.plot(np.arange(num_iterations), overall_opt_per4, label = \"increasing step-size with UCB\")\n",
        "plt.plot(np.arange(num_iterations), overall_opt_per5, label = \"Gradient-Bandit Algorithm\")\n",
        "plt.xticks([0, 2000, 4000, 6000, 8000, 10000], [\"0\", \"2000\", \"4000\", \"6000\", \"8000\", \"10000\"], fontsize = 12)\n",
        "plt.yticks([20, 40, 60, 80, 100], [\"20%\", \"40%\", \"60%\", \"80%\", \"100%\"], fontsize = 12)\n",
        "plt.xlabel(\"Steps\", fontsize = 15)\n",
        "plt.ylabel(\"%Optimal\\naction\", fontsize = 15, rotation=\"horizontal\", labelpad=40)\n",
        "plt.title(\"Average %Optimal_Action Over 2000 Runs\", fontsize = 20)\n",
        "plt.legend(fontsize = 16)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-p-P6NH-oOcW"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}