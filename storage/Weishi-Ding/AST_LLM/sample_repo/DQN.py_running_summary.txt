Here's the updated README.md with the appended information from the current code file description:

```markdown
**Summary**:
This Python code repository is dedicated to a reinforcement learning project focused on Q-learning models. The first reviewed file contains essential functions for data manipulation and processing, which are crucial for training and evaluating the neural network-based Q-learning algorithms. These functions facilitate the conversion of data into formats suitable for neural network processing, calculation of Q-values, and adjustment of date formats for consistent data handling. The addition of the DQN class, Experience named tuple, and Experience_Buffer class in the subsequent file further enriches the repository by introducing core components for the neural network model and experience management, essential for the effective training of Q-learning models.

**Technology Stack**:
- **Python**: The primary programming language used for implementing the functions.
- **NumPy**: Utilized for handling numerical operations on arrays.
- **PyTorch**: A deep learning framework employed to convert arrays into tensors, which are then used in neural network computations. This integration is crucial for leveraging GPU acceleration during the training process, enhancing the efficiency and scalability of the Q-learning models.

**Engineering Highlights**:
- The implementation of functions like `ndarray_to_tensor` and `oneDarray_to_tensor` demonstrates a thoughtful approach to data preprocessing, ensuring that data fed into the neural network is in the correct shape and format.
- The separation of functionality between calculating current Q-values (`get_cur_Qs`) and target Q-values (`get_target_Qs`) using different network models (policy and target networks) reflects a sophisticated understanding of reinforcement learning techniques, specifically the Double Q-learning algorithm, which helps in reducing overestimations of Q-values.
- The introduction of the `DQN` class and `Experience_Buffer` class highlights advanced engineering practices in managing neural network architectures and experience replay buffers, which are pivotal for the robustness and efficiency of the learning algorithms.

**Features**:
- **Data Transformation**: Functions such as `ndarray_to_tensor` and `oneDarray_to_tensor` transform NumPy arrays into PyTorch tensors, preparing the data for neural network processing.
- **Q-value Calculation**: The `get_cur_Qs` and `get_target_Qs` functions are pivotal in computing the Q-values needed for the Q-learning updates, facilitating the learning process of the agent.
- **Date Format Standardization**: The `adjust_date_format` function ensures that all date data within the project adheres to a uniform format (YYYY-MM-DD), which is crucial for maintaining consistency in data processing and logging.
- **Neural Network Model**: The `DQN` class encapsulates the neural network architecture specific to Q-learning, equipped with methods for initializing and executing forward passes.
- **Experience Management**: The `Experience_Buffer` class provides mechanisms for storing and sampling experiences, crucial for implementing experience replay which enhances training stability.

**Usage**:
- The function `ndarray_to_tensor(array, batch_size, num_features)` is used to convert state and action arrays into tensors, reshaping them according to the batch size and number of features required by the neural network. This is essential for batch processing during neural network training.
- The `get_cur_Qs(policy_net, states, actions)` function utilizes the policy network to calculate the current Q-values for given states and actions, playing a critical role in the agent's decision-making process during training and evaluation phases.
- The `DQN` class is instantiated with specific input and output dimensions to match the requirements of the Q-learning task, and its `forward` method is used to compute the output from given input states.
- The `Experience_Buffer` is used to manage the experiences collected during training, with functionalities to add new experiences and sample batches of experiences for training the DQN model.
```

This updated README.md now includes comprehensive details from the current code file, enhancing the overall documentation of the Python code repository.